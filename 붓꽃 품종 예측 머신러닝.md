## 머신러닝

---

- 붓꽃 품종 예측하기
  1. 이진 분류 모델 (DecisionTreeClassifier)

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier #트리관련 알고리즘
from sklearn.model_selection import train_test_split #train,test data 분류하기 위해
from sklearn.metrics import accuracy_score #성능평가

import pandas as pd

#붓꽃 데이터 세트 로딩
iris=load_iris()

#iris data : feature
iris_data=iris.data
print('iris feature값 :',iris_data)
#iris target : label
iris_label=iris.target
print('iris target값 :',iris_label)
print('iris target명:',iris.target_names)
```

```python
#train data 와 test data 분리
#편향되지 않은 성능을 측정하기 위해서
x_train,x_test,y_train,y_test=train_test_split(iris_data,iris_label,test_size=0.3,random_state=12)
#split은 문자열을 나누어준다
#20프로 잘라주는 것

#DecisionTreeClassifier 객체 생성
df_clf=DecisionTreeClassifier(random_state=11)
#학습 수행 fit 
df_clf.fit(x_train,y_train)
#예측 수행 predict
pred=df_clf.predict(x_test)
#성능 평가 accuracy, 정확도
print('예측 정확도 : {0:4f}'.format(accuracy_score(y_test,pred)))
```

2. KFold

   ```python
   from sklearn.tree import DecisionTreeClassifier
   from sklearn.metrics import accuracy_score
   from sklearn.model_selection import KFold
   import numpy as np
   
   iris=load_iris()
   features=iris.data
   label=iris.target
   dt_clf=DecisionTreeClassifier(random_state=156) #씨드
   
   #5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성
   kfold=KFold(n_splits=5)
   cv_accuracy=[]
   print('붓꽃 데이터 세트 크기 :',features.shape[0]) #150개 출력
   
   n_iter=0
   
   #KFold 객체의 split()를 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환
   for train_index, test_index in kfold.split(features):
       #kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출
       x_train,x_test= features[train_index], features[test_index]
       y_train,y_test= label[train_index],label[test_index]
       
       #학습 및 예측
       dt_clf.fit(x_train,y_train) #학습
       pred=dt_clf.predict(x_test) #예측
       n_iter +=1
       
       #반복 시마다 정확도 측정
       accuracy=np.round(accuracy_score(y_test,pred),4)
       #소수점 설정
       train_size=x_train.shape[0] #행만 추출
       test_size=x_test.shape[0]
       
       print('\n#{0}교차 검증 정확도 : {1}, 학습 데이터 크기 : {2},검증 데이터 크기 : {3}'.format(n_iter,accuracy,train_size,test_size))
       cv_accuracy.append(accuracy)
   #개별 interaction 별 정확도를 합하여 평균 정확도 계산
   print('\n## 평균 검증 정확도 :', np.mean(cv_accuracy))
   ```

   ```python
   import pandas as pd
   iris=load_iris()
   
   iris_df=pd.DataFrame(data=iris.data,columns=iris.feature_names)
   iris_df['label']=iris.target
   iris_df['label'].value_counts()
   
   kfold=KFold(n_splits=3)
   #kflod.split(x)는 폴드 세트를 3번 반복할 때마다 달라지는 학습, 테스트 용 데이터 로우 인덱스 번호 반환
   
   n_iter=0
   for train_index,test_index in kfold.split(iris_df):
       n_iter +=1
       label_train = iris_df['label'].iloc[train_index]
       label_test=iris_df['label'].iloc[test_index]
       print('##교차 검증 :{0}'.format(n_iter))
       print('학습 레이블 데이터 분포:\n',label_train.value_counts())
       print('검증 레이블 데이터 분포:\n',label_test.value_counts())
   ```

   3. StratifiedKFold

      데이터의 편향을 막기 위해 사용 

   ```python
   from sklearn.model_selection import StratifiedKFold
   
   skf=StratifiedKFold(n_splits=3) #3개 폴드 세트 추출
   n_iter=0
   
   #섞어서 인덱스를 놓아준 것
   for train_index, test_index in skf.split(iris_df,iris_df['label']):
       n_iter +=1
       label_train=iris_df['label'].iloc[train_index]
       label_test=iris_df['label'].iloc[test_index]
       print('##교차 검증 :{0}'.format(n_iter))
       print('학습 레이블 데이터 분포:\n', label_train.value_counts()) #학습 데이터
       print('검증 레이블 데이터 분포:\n', label_test.value_counts()) #검정 데이터
       #골고루 분포해있는 것을 알 수 있음
       #교차검증
   dt_clf=DecisionTreeClassifier(random_state=156) #씨드값
   
   skfold=StratifiedKFold(n_splits=3)
   n_iter=0
   cv_accuracy=[]
   
   #split() 호출 시 반드시 레이블 데이터 셋도 추가 입력 필요
   for train_index,test_index in skfold.split(features,label):
       # split()으로 반환된 인덱스를 이용하여 train, test 데이터 추출
       x_train, x_test=features[train_index],features[test_index]
       y_train, y_test=label[train_index],label[test_index]
       
       #학습 및 예측
       dt_clf.fit(x_train,y_train)
       pred=dt_clf.predict(x_test)
       
       #반복 시 마다 정확도 측정
       n_iter +=1
       accuracy=np.round(accuracy_score(y_test,pred),4)
       train_size=x_train.shape[0]
       test_size=x_test.shape[0]
       
       print('\n#{0}교차 검증 정확도 : {1}, 학습 데이터 크기 : {2}, 검증 데이터 크기 :{3}'
            .format(n_iter,accuracy,train_size,test_size))
       print('#{0} 검증 세트 인덱스 :{1}'.format(n_iter,test_index))
       
       cv_accuracy.append(accuracy)
       
   #교차 검증별 정확도 및 평균 정확도 계산
   print('\n##교차 검증별 정확도 :', np.round(cv_accuracy,4))
   print('## 평균 검증 정확도:', np.mean(cv_accuracy))
   ```

   4. Cross_val_score()

   간편한 교차검증, 하나의 평가지표만 사용

   cross_validate 는 여러개의 평가지표

   ```python
   from sklearn.tree import DecisionTreeClassifier
   from sklearn.model_selection import cross_val_score, cross_validate
   from sklearn.datasets import load_iris
   
   iris_data=load_iris()
   dt_clf=DecisionTreeClassifier(random_state=156)
   
   data=iris_data.data
   label=iris_data.target
   
   scores=cross_val_score(dt_clf,data,label,scoring='accuracy',cv=3)
   print('교차 검증별 정확도:',np.round(scores,4))
   print('평균 검증 정확도:',np.round(np.mean(scores),4))
   
   iris_data=load_iris()
   
   x_train,x_test,y_train,y_test=train_test_split(iris_data.data,iris_data.target,
                                                 test_size=0.2,random_state=121)
   dtree=DecisionTreeClassifier()
   
   parameters={'max_depth':[1,2,3],'min_samples_split':[2,3]}
   
   #GridSearchCV : 교차검증+하이퍼 파라미터 튜닝 : 과적합 방지
   import pandas as pd
   
   #param_grid의 하이퍼 파라미터들을 3개의 train, test set fold로 나누어서 테스트 수행 설정
   #refit=True 가 default
   #TRUE 일때 가장 좋은 파라미터 설정으로 재학습시킴
   grid_dtree=GridSearchCV(dtree,param_grid=parameters,cv=3,refit=True) #kfold 3으로 처리
   
   #붓꽃 Train data 로 param_grid의 하이퍼 파라미터들을 순차적으로 학습, 평가
   grid_dtree.fit(x_train,y_train)
   
   #GridSearchCV 결과 추출하여 DataFrame으로 변환
   scores_df=pd.DataFrame(grid_dtree.cv_results_)
   scores_df[['params','mean_test_score','rank_test_score',\
              'split0_test_score','split1_test_score','split2_test_score']]
   
   #트리구조-> 과적합 위험
   #depth를 끊어서 과적합 방지, 4개 정도로 트레이닝 시키기
   
   print('GridSearchCV 최적 파라미터:',grid_dtree.best_params_)
   print('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_dtree.best_score_))
   
   #GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
   estimator=grid_dtree.best_estimator_
   
   #GridSearchCV의 best
   pred=estimator.predict(x_test)
   print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))
   ```

   5. 원 핫 인코딩

   원본->레이블 인코딩->원 핫 인코딩

   레이블 인코딩이 숫자의 크고 작음이 있는 값으로 변환되면서 특정 알고리즘에서 가중치가 더 부여되는 문제가 발생

   트리 계열의 알고리즘은 문제가 없으나 선형회귀 알고리즘은 문제 발생

   원 핫 인코딩이 문제들을 해결

   ```python
   from sklearn.preprocessing import LabelEncoder
   from sklearn.preprocessing import OneHotEncoder
   
   #LabelEncoder를 객체로 생성한 후, fit, transform으로 레이블인코딩수행
   items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']
   
   encoder=LabelEncoder()
   encoder.fit(items)
   labels=encoder.transform(items)
   
   #2차원 데이터로 변환
   labels=labels.reshape(-1,1) #행,열
   
   #원-핫 인코딩 적용
   oh_encoder=OneHotEncoder()
   oh_encoder.fit(labels)
   oh_labels=oh_encoder.transform(labels)
   print('원-핫 인코딩 데이터')
   print(oh_labels.toarray())#데이터 배열로 나타내도록 함
   print('원-핫 인코딩 데이터 차원')
   print(oh_labels.shape) #8행 6열
   ```

   원 핫 인코딩을 쉽게 할 수 있는 방법 : get_dummies

   ```python
   import pandas as pd
   df=pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']})
   pd.get_dummies(df)
   ```

   6. feature scaling

   두 변수 중 하나의 값이 너무 클 경우 학습을 시킬 때 가중치가 불균형하게 반영이 될 수 있음

   학습 시 속도를 고려해서 값을 축소

   ```python
   from sklearn.datasets import load_iris
   import pandas as pd
   
   #붓꽃 데이터 세트 로딩하고 데이터 프레임으로 변환
   iris=load_iris()
   iris_data=iris.data
   iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names)
   
   print('feature들의 평균 값')
   print(iris_df.mean())
   print('\nfeature 들의 분산 값')
   print(iris_df.var())
   ```

   7. 표준화 변환

   특성의 평균을 0으로 맞추고 표준편자를 1로 만들어 정규분포와 같은 특징을 가지도록 만듬

   가중치를 더 쉽게 학습

   ```python
   from sklearn.preprocessing import StandardScaler
   
   scaler=StandardScaler()
   #StandardScaler로 데이터 셋 변환, fit, transform 호출
   scaler.fit(iris_df)
   iris_scaled=scaler.transform(iris_df)
   
   #transform()시 scale 변환된 데이터 셋이 numpy 로 변환되어 데이터 프레임으로 변환
   iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
   print('feature들의 평균 값')
   print(iris_df_scaled.mean())
   print('\nfeature 들의 분산 값')
   print(iris_df_scaled.var())
   ```

   8. 정규화

   정해진 범위의 값이 필요할 때 사용

   ```python
   from sklearn.preprocessing import MinMaxScaler
   
   scaler=MinMaxScaler()
   
   scaler.fit(iris_df)
   iris_scaled=scaler.transform(iris_df)
   
   iris_df_scaled=pd.DataFrame(data=iris_scaled,columns=iris.feature_names)
   print('feature들의 최소 값')
   print(iris_df_scaled.min())
   print('\nfeature들의 최대 값')
   print(iris_df_scaled.max())
   #0에서 1사이의 값으로 변환
   ```

   

   - 머신러닝 학습, 예측, 성능 평가

   ```python
   #scaler 객체를 이용해서 데이터 변환 시에 fit, transform
   from sklearn.preprocessing import MinMaxScaler
   import numpy as np
   
   #학습 데이터는 0부터 10까지
   train_array=np.arange(0,11).reshape(-1,1)
   #테스트 데이터는 0부터 5까지 값을 가지는 데이터세트 형성
   test_array=np.arange(0,6).reshape(-1,1)
   
   #최소값 0, 최대값 1로 변환하는 MinMaxScaler 객체 생성
   scaler=MinMaxScaler()
   
   #fit하게 되면 train_array 데이터의 최소값이 0 최대값 10으로 설정
   scaler.fit(train_array)
   
   train_scaled=scaler.transform(train_array)
   
   print('원본 train_array data:',np.round(train_array.reshape(-1),2))
   print('Scaled train_array data:',np.round(train_scaled.reshape(-1),2))
   
   #MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최소값이 0 최대값이 5로 설정
   scaler.fit(test_array)
   
   #1/5로 test_array 데이터 변환
   test_scaled=scaler.transform(test_array)
   
   print('원본 test_array data:',np.round(test_array.reshape(-1),2))
   print('scaled test_array data:',np.round(test_scaled.reshape(-1),2))
   ```

   